# Leaderboard Implementation Summary

## âœ… Implementation Complete

All components for the PeerBench leaderboard calculation system have been implemented!

## ğŸ“ Files Created

### Core Library (`src/lib/leaderboard/`)
- âœ… `types.ts` - TypeScript types and interfaces
- âœ… `contributor-scoring.ts` - Contributor scoring algorithm
- âœ… `reviewer-scoring.ts` - Reviewer scoring algorithm (Pearson correlation)
- âœ… `simulation.ts` - Virtual user/prompt/feedback generator
- âœ… `stats.ts` - Statistics calculator
- âœ… `data-fetcher.ts` - Server action to fetch real data from DB
- âœ… `index.ts` - Main orchestrator and exports

### React Components (`src/components/leaderboard/`)
- âœ… `DataSourceSelector.tsx` - Choose real vs simulated data
- âœ… `SimulationConfig.tsx` - Configure and run simulation
- âœ… `CoefficientEditor.tsx` - Edit algorithm coefficients
- âœ… `StatsDisplay.tsx` - Display input data statistics
- âœ… `ContributorLeaderboard.tsx` - Display contributor rankings
- âœ… `ReviewerLeaderboard.tsx` - Display reviewer rankings
- âœ… `index.ts` - Component exports

### Admin Page (`src/app/simScores001/`)
- âœ… `page.tsx` - Main admin page (single page, client-side)

### Documentation (`docs/algo/scores0001/`)
- âœ… `README.md` - Complete algorithm documentation with math formulas
- âœ… `IMPLEMENTATION_SUMMARY.md` - This file

## ğŸš€ How to Use

### 1. Access the Admin Page

Navigate to: **`/simScores001`**

### 2. Choose Data Source

**Option A: Real Database Data**
- Fetches current prompts and feedbacks from database
- Uses actual user affiliations
- Good for testing with real data

**Option B: Simulate New Data**
- Configure simulation parameters:
  - Number of users
  - User persona distribution (altruistic, greedy, cabal, random, malicious)
  - Number of prompts per user
  - Review probabilities
  - Percentage of good prompts
  - Cabal size
- Run simulation to generate virtual data
- Good for algorithm testing and experimentation

### 3. Configure Coefficients

Edit algorithm parameters:
- **Contributor Scoring:**
  - Affiliation Bonus Points (default: 10)
  - Quality Weight (default: 0.7)
  - Reputation Weight (default: 0.3)
  - Reputation Cap (default: 2)
  - Minimum Reviews for Quality (default: 3)

- **Reviewer Scoring:**
  - Minimum Reviews Required (default: 5)

### 4. Calculate Leaderboards

Click **"Calculate Leaderboards"** button
- All calculations happen client-side in browser
- Results displayed immediately
- No database writes

### 5. View Results

**Statistics Panel:**
- Total users, prompts, feedbacks
- Distribution of reviews per prompt
- Reviewer statistics
- Opinion distribution (positive/negative)

**Contributor Leaderboard:**
- Ranked by total score
- Shows quality score, affiliation bonus, prompt count, avg quality
- Top 3 highlighted with medals ğŸ¥‡ğŸ¥ˆğŸ¥‰

**Reviewer Leaderboard:**
- Ranked by Pearson correlation
- Shows alignment quality (Excellent/Good/Fair/Poor)
- Review count

### 6. Export Results

Click **"Export JSON"** to download full results including:
- Both leaderboards
- Statistics
- Coefficients used
- Timestamp

## ğŸ­ User Personas (Simulation)

1. **Altruistic** - Honest reviews based on actual prompt quality
2. **Greedy** - Only upvotes own prompts, ignores others
3. **Cabal** - Groups that upvote each other, downvote outsiders
4. **Random** - Random opinions regardless of quality
5. **Malicious** - Intentionally gives wrong reviews

## ğŸ”¬ Algorithm Details

### Contributor Score Formula
```
ContributorScore = Î£(prompt_quality_scores) + affiliation_bonus
```

Where prompt quality is:
```
quality = Î£(opinion Ã— reputation) / Î£(reputation)
```
- opinion: +1 (positive) or -1 (negative)
- reputation: currently 1.0 for all reviewers
- Minimum 3 reviews required

### Reviewer Score Formula
```
ReviewerScore = Pearson_Correlation(reviewer_ratings, consensus_ratings)
```

Pearson correlation range:
- **+1.0**: Perfect agreement
- **+0.7 to +1.0**: Excellent
- **+0.3 to +0.7**: Good
- **0 to +0.3**: Fair
- **Below 0**: Poor (disagrees)

## ğŸ’¾ Data Storage

**Phase 1 (Current):**
- âœ… Zero database schema changes
- âœ… All calculations client-side
- âœ… Results stored only in browser state
- âœ… Can export to JSON

**Future Phases:**
- Database tables for persisting scores
- Historical tracking
- Public leaderboards
- Automated recalculation

## ğŸ› Known Limitations

1. **Reviewer Reputation**: Currently fixed at 1.0 for all reviewers. Future: iterative updates.
2. **Binary Opinions**: Only positive/negative. Paper uses -1, 0, 1, 2 scale.
3. **No Test Weighting**: Paper formula for prompt weights not yet used.
4. **Client-Side Only**: Everything in browser, no persistence.
5. **Bootstrap Problem**: Need scores to calculate reputations, need reputations to calculate scores.

## ğŸ“Š Example Use Cases

### Testing Algorithm Changes
1. Run simulation with default parameters
2. Calculate scores and export baseline
3. Adjust coefficients (e.g., change affiliation bonus)
4. Recalculate and compare results

### Comparing Persona Behaviors
1. Run simulation with 100% altruistic users
2. Calculate and export
3. Run simulation with 50% altruistic, 50% malicious
4. Calculate and compare how malicious users affect scores

### Validating with Real Data
1. Select "Real Database Data"
2. Calculate leaderboards
3. Check statistics to understand current data quality
4. Identify prompts needing more reviews

## ğŸ” Debugging

The page logs detailed statistics to the browser console:
- User counts
- Prompt review distribution
- Reviewer statistics
- Opinion distribution

Check console with F12 â†’ Console tab.

## ğŸ¯ Next Steps

**Immediate:**
- [x] Test with simulated data
- [ ] Test with real database data (if available)
- [ ] Adjust default coefficients based on testing
- [ ] Gather feedback on algorithm behavior

**Future:**
- [ ] Add database persistence
- [ ] Implement iterative reputation updates
- [ ] Add time decay for reviews
- [ ] Cabal detection algorithms
- [ ] Public leaderboard pages
- [ ] Automated score updates

## ğŸ“š Documentation

Full algorithm documentation with mathematical formulas and examples:
- **Location**: `docs/algo/scores0001/README.md`
- **Contents**: Formulas, examples, edge cases, future improvements

## ğŸ‰ Ready to Use!

Navigate to **`/simScores001`** and start experimenting with the leaderboard algorithms!
